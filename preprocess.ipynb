{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "def casting(df, features): \n",
    "    df[features] = df[features].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define mappings that will be used throughout pre processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_mappings = {\n",
    "    'BLD 2': 'BLD 02',\n",
    "    'BLD 8': 'BLD 08',\n",
    "    'BLD 7': 'BLD 07',\n",
    "    'BLD 1': 'BLD 10'\n",
    "}\n",
    "\n",
    "type_float_mapping = {\n",
    "    'Current Charges': float,\n",
    "    'Other Charges': float,\n",
    "    'Consumption (HCF)': float,\n",
    "    '# days': float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34627 entries, 0 to 34626\n",
      "Data columns (total 25 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Development Name     34620 non-null  object \n",
      " 1   Borough              34627 non-null  object \n",
      " 2   Account Name         34627 non-null  object \n",
      " 3   Location             34365 non-null  object \n",
      " 4   Meter AMR            34575 non-null  object \n",
      " 5   Meter Scope          8585 non-null   object \n",
      " 6   TDS #                34620 non-null  float64\n",
      " 7   EDP                  34627 non-null  int64  \n",
      " 8   RC Code              34627 non-null  object \n",
      " 9   Funding Source       34551 non-null  object \n",
      " 10  AMP #                34618 non-null  object \n",
      " 11  Vendor Name          34627 non-null  object \n",
      " 12  UMIS BILL ID         34627 non-null  int64  \n",
      " 13  Revenue Month        34627 non-null  object \n",
      " 14  Service Start Date   34627 non-null  object \n",
      " 15  Service End Date     34627 non-null  object \n",
      " 16  # days               34627 non-null  object \n",
      " 17  Meter Number         34627 non-null  object \n",
      " 18  Estimated            34627 non-null  object \n",
      " 19  Current Charges      34627 non-null  object \n",
      " 20  Rate Class           34624 non-null  object \n",
      " 21  Bill Analyzed        34627 non-null  object \n",
      " 22  Consumption (HCF)    34627 non-null  object \n",
      " 23  Water&Sewer Charges  34627 non-null  object \n",
      " 24  Other Charges        34627 non-null  object \n",
      "dtypes: float64(1), int64(2), object(22)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be deleted\n",
    "del_cols = [\n",
    "    'Development Name', 'EDP', 'Vendor Name', 'Meter Scope',\n",
    "    'AMP #', 'Water&Sewer Charges', 'UMIS BILL ID', 'RC Code'\n",
    "] \n",
    "\n",
    "bool_cols = ['Meter AMR', 'Bill Analyzed', 'Estimated', 'Funding Source', 'Rate Class',]\n",
    "strnum_cols = ['Current Charges', 'Other Charges', 'Consumption (HCF)', '# days']\n",
    "loc_col  = ['Location']\n",
    "meter_number_col = ['Meter Number']\n",
    "tds_cols = ['TDS #']\n",
    "\n",
    "pre_proc_cols = bool_cols + strnum_cols + loc_col + meter_number_col + tds_cols\n",
    "\n",
    "# Columns to keep. Since column transformer makes a mess with the attributes of DataFrame\n",
    "# this trick will be used to retrieve the correct column's names once the transformer has finished\n",
    "keep_cols = dataset.columns.difference(set.union(set(del_cols), set(pre_proc_cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, special transformers are constructed to apply pre-processing operations according to dict.xlsx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary operations\n",
    "### P04 and P05\n",
    "\n",
    "This transformer takes in input the DataFrame with the columns ['Meter AMR', 'Bill Analyzed', 'Estimated'] and applies a boolean transformations of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_transformer = FunctionTransformer(\n",
    "    func=lambda df, levels: df == levels,\n",
    "    kw_args={'levels': ['AMR', 'Yes', 'Y', 'FEDERAL', 'Basic Water and Sewer']},\n",
    "    feature_names_out='one-to-one'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P09\n",
    "\n",
    "The following transformer maps every string where numbers >= 1000 are represented with a comma, into a classic decimal number. For every string-number where the char \",\" is present, the transformer: \n",
    "1. Splits when \",\" is encountered;\n",
    "2. Joins the resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "strnum_transformer = FunctionTransformer(\n",
    "    func=lambda df: df.map(lambda v: ''.join(v.split(',')) if ',' in v else v),\n",
    "    feature_names_out='one-to-one'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P06\n",
    "\n",
    "The transformers here uses the function `fill_null_locations` in order to map every different null locations with a progressive number that corresponds to the index of the meter. \n",
    "\n",
    "Note: even if tecnically the function should operate on a series, a DataFrame is forced in order to not mess with `ColumnTransformer` later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_null_locations(location_df: pd.DataFrame, df_null_location: pd.DataFrame):\n",
    "    # Use the sub-dataframe to obtain the indices of each unique meter number associated with a null location\n",
    "    meter_number_null_location_idx = df_null_location[['Meter Number', 'TDS #']].value_counts().reset_index()\n",
    "\n",
    "    # Define the mapping as it is described in the documentation\n",
    "    location_map = {\n",
    "        # This is used to perform an inverse mapping of a dictionary\n",
    "        v: f'loc_{k + 1}' for k, v in meter_number_null_location_idx['Meter Number'].to_dict().items()\n",
    "    }\n",
    "\n",
    "    # Fill the null locations by mapping the associated meter number with its index\n",
    "    location_df.loc[location_df['Location'].isna(), 'Location'] = df_null_location['Meter Number'].map(location_map)\n",
    "    \n",
    "    return location_df\n",
    "\n",
    "location_imputer = FunctionTransformer(\n",
    "    func=fill_null_locations,\n",
    "    kw_args={'df_null_location': dataset[dataset['Location'].isna()]},\n",
    "    feature_names_out='one-to-one'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P07\n",
    "\n",
    "To fix the locations names a very straightforward mapping it is applied, using the previously defined dictionary `loc_mappings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_location_names_transformer = FunctionTransformer(\n",
    "    func=lambda df: df[['Location']].map(lambda x: x if x not in loc_mappings.keys() else loc_mappings[x]), \n",
    "    feature_names_out='one-to-one'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P09\n",
    "\n",
    "The strategy to impute unknown meter is very similar to P06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_un_metered(meter_df: pd.DataFrame, df_un_metered: pd.DataFrame):\n",
    "    # Add a building column to the input sub-df\n",
    "    df_un_metered['Building'] = df_un_metered['TDS #'].astype(str) + '_' + df_un_metered['Location']\n",
    "\n",
    "    # Use the sub dataframe to obtain the indices that will be assigned to distinguish un-metered cases\n",
    "    un_metered_idx = df_un_metered[['Meter Number', 'Building']].value_counts().reset_index()\n",
    "\n",
    "    # Define the mapping using the new building column\n",
    "    meter_map = {\n",
    "        v: f'meter_{k + 1}' for k, v in un_metered_idx['Building'].to_dict().items()\n",
    "    }\n",
    "\n",
    "    meter_df.loc[meter_df['Meter Number'] =='UN-METERED', 'Meter Number'] = df_un_metered['Building'].map(meter_map)\n",
    "\n",
    "    return meter_df\n",
    "\n",
    "meter_imputer = FunctionTransformer(\n",
    "    func=fix_un_metered,\n",
    "    kw_args={'df_un_metered': dataset[dataset['Meter Number'] =='UN-METERED']},\n",
    "    feature_names_out='one-to-one'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pipeline to perform the operations on the location column\n",
    "location_pipeline = Pipeline([\n",
    "    ('nan_loc', location_imputer),\n",
    "    ('loc_names', fix_location_names_transformer)\n",
    "])\n",
    "\n",
    "tds_imputer = SimpleImputer(strategy='constant', fill_value=999)\n",
    "\n",
    "# Define the column transformer to wrap the defined transformations into a single object\n",
    "encoder_col_transf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('drop_cols', 'drop', del_cols), # P01\n",
    "        ('keep_cols', 'passthrough', keep_cols), # define the features to be kept without transformations\n",
    "        ('bool_features', bool_transformer, bool_cols), # P04 and P05\n",
    "        ('str_to_float', strnum_transformer, strnum_cols), # P09\n",
    "        ('location_transformation', location_pipeline, loc_col), # P06\n",
    "        ('meter_imputation', meter_imputer, meter_number_col), # P08\n",
    "        ('tds_imputation', tds_imputer, tds_cols), # P02\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple pre processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_deleter = FunctionTransformer(func=lambda x: x.drop(index=[4785, 8156]), feature_names_out='one-to-one')\n",
    "\n",
    "preprocessing_pipe = Pipeline([\n",
    "    ('rows_del', row_deleter), # P10\n",
    "    ('col_transf', encoder_col_transf),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34625 entries, 0 to 34624\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Account Name        34625 non-null  string \n",
      " 1   Borough             34625 non-null  string \n",
      " 2   Revenue Month       34625 non-null  string \n",
      " 3   Service End Date    34625 non-null  string \n",
      " 4   Service Start Date  34625 non-null  string \n",
      " 5   Meter AMR           34625 non-null  boolean\n",
      " 6   Bill Analyzed       34625 non-null  boolean\n",
      " 7   Estimated           34625 non-null  boolean\n",
      " 8   is_federal          34625 non-null  boolean\n",
      " 9   is_bws              34625 non-null  boolean\n",
      " 10  Current Charges     34625 non-null  Float64\n",
      " 11  Other Charges       34625 non-null  Float64\n",
      " 12  Consumption (HCF)   34625 non-null  Int64  \n",
      " 13  # days              34625 non-null  Int64  \n",
      " 14  Location            34625 non-null  string \n",
      " 15  Meter Number        34625 non-null  string \n",
      " 16  TDS #               34625 non-null  Int64  \n",
      "dtypes: Float64(2), Int64(3), boolean(5), string(7)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "clean_dataset = pd.DataFrame(\n",
    "    preprocessing_pipe.fit_transform(dataset), \n",
    "    columns=encoder_col_transf.get_feature_names_out()\n",
    ")\n",
    "\n",
    "real_col_names = [col[-1] for col in clean_dataset.columns.str.split('__')]\n",
    "\n",
    "clean_dataset = clean_dataset \\\n",
    "                    .rename(columns=pd.Series(real_col_names, index=clean_dataset.columns).to_dict()) \\\n",
    "                    .astype(type_float_mapping) \\\n",
    "                    .convert_dtypes() \\\n",
    "                    .rename(columns={'Funding Source': 'is_federal', 'Rate Class': 'is_bws'})\n",
    "\n",
    "clean_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning meter into unit of analysis\n",
    "\n",
    "After the pre processing made, a meter cannot be associated anymore with multiple buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dataset[['Meter Number', 'TDS #', 'Location']].value_counts().shape[0] == clean_dataset['Meter Number'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define aggregations using dictionaries. To see the deepests reasons behind these operations look dict.xlsx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_agg_not_strings = ['TDS #', 'is_federal', 'is_bws', 'Meter AMR']\n",
    "lonely_columns = ['Revenue Month', 'Service End Date', 'Meter Number']\n",
    "\n",
    "first_agg_cols = clean_dataset.select_dtypes(include='string').columns.difference(lonely_columns).append(pd.Index(first_agg_not_strings))\n",
    "sum_agg_cols = clean_dataset.select_dtypes(include=['number', 'bool']).columns.difference(first_agg_not_strings)\n",
    "\n",
    "first_agg_mapping = {value: 'first' for value in first_agg_cols}\n",
    "sum_agg_mapping = {value: lambda x: round(abs(sum(x)), 2) for value in sum_agg_cols}\n",
    "other_agg_mapping = {lonely_columns[1]: 'last', lonely_columns[0]: 'count'}\n",
    "\n",
    "aggregation_operations = first_agg_mapping | sum_agg_mapping | other_agg_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the groupby can be performed safely. The resulting DataFrame will be of about 700 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator_transformer = FunctionTransformer(\n",
    "    func=lambda df, agg_map: (\n",
    "        df.groupby(by='Meter Number')\n",
    "            .aggregate(agg_map)\n",
    "            .rename(columns={'Revenue Month': 'Times read'})\n",
    "            .reset_index(drop=False)), \n",
    "    kw_args={'agg_map': aggregation_operations}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the aggregation is performed, TDS and Location can be finally merged into a unique features called **buiding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_building(agg_df: pd.DataFrame):\n",
    "    agg_df['Building'] = agg_df['TDS #'].astype(str) + '_' + agg_df['Location']\n",
    "    agg_df = agg_df.drop(columns=(['Location', 'TDS #']))\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "def create_score_columns(agg_df: pd.DataFrame):\n",
    "    agg_df['Estimated score'] = np.abs((agg_df['Estimated'] / agg_df['Times read']))\n",
    "    agg_df['Bill Analyzed score'] = np.abs((agg_df['Bill Analyzed'] / agg_df['Times read']))\n",
    "    \n",
    "    agg_df = agg_df.drop(columns=['Estimated', 'Times read', 'Bill Analyzed'])\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "building_creator = FunctionTransformer(func=create_building)\n",
    "score_creator = FunctionTransformer(func=create_score_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline to perform these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_pipeline = Pipeline([\n",
    "    ('aggregation', aggregator_transformer),\n",
    "    ('building', building_creator),\n",
    "    ('score', score_creator)\n",
    "])\n",
    "\n",
    "agg_dataset = aggregation_pipeline \\\n",
    "                    .fit_transform(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this version of the dataset for further explorations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dataset.to_csv('./data/agg_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After aggregation operations\n",
    "\n",
    "### AP03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_transformer = FunctionTransformer(\n",
    "    func=lambda df: df[['Borough']].map(lambda v: 'QUEENS' if v == 'FHA' else v),\n",
    "    feature_names_out='one-to-one'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_cat_getter = FunctionTransformer(\n",
    "    func=lambda df: df[['Building']].map(\n",
    "                            lambda x: \n",
    "                                'STREET' if 'STREET' in x else \n",
    "                                'AVENUE' if 'AVENUE' in x else\n",
    "                                'PLACE' if 'PLACE' in x else \n",
    "                                'COMMUNITY CENTER' if 'Community Center' in x else\n",
    "                                'BOULEVARD' if 'BOULEVARD' in x else\n",
    "                                'ROAD' if 'ROAD' in x else \n",
    "                                'UNSPECIFIED_TYPE' # default\n",
    "                            ),\n",
    "    feature_names_out='one-to-one'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_type_getter = FunctionTransformer(\n",
    "    func=lambda df: df[['Account Name']].map(\n",
    "                                    lambda x: \n",
    "                                        'FHA' if 'FHA' in x else \n",
    "                                        'REHAB' if 'REHAB' in x else \n",
    "                                        'UNSPECIFIED_CATEGORY' # default\n",
    "                                    ),\n",
    "    feature_names_out='one-to-one'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_date(s: pd.Series):\n",
    "    splitting = s.str.split('/')\n",
    "    date_feature_name = s.name\n",
    "\n",
    "    df = pd.DataFrame(s) # the series is turned into a DataFrame\n",
    "\n",
    "    df[f'month_{date_feature_name}'] = splitting.map(lambda x: x [0])\n",
    "    df[f'day_{date_feature_name}'] = splitting.map(lambda x: x[1])\n",
    "    df[f'year_{date_feature_name}'] = splitting.map(lambda x: x[2])\n",
    "\n",
    "    df = df.drop(columns=[date_feature_name])\n",
    "\n",
    "    return df\n",
    "\n",
    "date_divider = FunctionTransformer(func=split_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column transformer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applied for clustering dataset\n",
    "mappings_col_transf_clustering = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('del_meter_numer', 'drop', 'Meter Number'),\n",
    "        ('boroughs_mapping', borough_transformer, ['Borough']),\n",
    "        ('location_category', location_cat_getter, ['Building']),\n",
    "        ('development_type', development_type_getter, ['Account Name']),\n",
    "        ('other_charges_bool', bool_transformer, ['Other Charges']),\n",
    "        ('date_end_divider', date_divider, 'Service Start Date'),\n",
    "        ('date_start_divider', date_divider, 'Service End Date'),\n",
    "        ('del_dates', 'drop', ['Service Start Date', 'Service End Date']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "# Applied to perform association analysis\n",
    "mappings_col_transf_association = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('del_meter_numer', 'drop', 'Meter Number'),\n",
    "        ('boroughs_mapping', borough_transformer, ['Borough']),\n",
    "        ('location_category', location_cat_getter, ['Building']),\n",
    "        ('development_type', development_type_getter, ['Account Name']),\n",
    "        ('other_charges_bool', bool_transformer.set_params(kw_args={'levels':0}), ['Other Charges']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ").set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following encodings are meant for clustering purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_enc_features = [\n",
    "    'boroughs_mapping__Borough', \n",
    "    'location_category__Building', \n",
    "    'development_type__Account Name'\n",
    "]\n",
    "\n",
    "cyclic_enc_features = [\n",
    "    'date_end_divider__month_Service Start Date', \n",
    "    'date_end_divider__day_Service Start Date',\n",
    "    'date_start_divider__month_Service End Date',\n",
    "    'date_start_divider__day_Service End Date'\n",
    "]\n",
    "\n",
    "enc_col_transf_clustering = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('count_enc', CountFrequencyEncoder(encoding_method='frequency'), count_enc_features),\n",
    "        ('cyclic_enc', CyclicalFeatures(drop_original=True), cyclic_enc_features)\n",
    "    ], \n",
    "    remainder='passthrough'\n",
    ").set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_preproc_pipeline = Pipeline([\n",
    "    ('mappings', mappings_col_transf_clustering),\n",
    "    ('cast', FunctionTransformer(func=casting, kw_args={'features': cyclic_enc_features})),\n",
    "    ('encoding', enc_col_transf_clustering)\n",
    "])\n",
    "\n",
    "agg_dataset_clustering = clustering_preproc_pipeline.fit_transform(agg_dataset).convert_dtypes()\n",
    "agg_dataset_association = mappings_col_transf_association.fit_transform(agg_dataset).convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 742 entries, 0 to 741\n",
      "Data columns (total 22 columns):\n",
      " #   Column                                                      Non-Null Count  Dtype  \n",
      "---  ------                                                      --------------  -----  \n",
      " 0   count_enc__boroughs_mapping__Borough                        742 non-null    Float64\n",
      " 1   count_enc__location_category__Building                      742 non-null    Float64\n",
      " 2   count_enc__development_type__Account Name                   742 non-null    Float64\n",
      " 3   cyclic_enc__date_end_divider__month_Service Start Date_sin  742 non-null    Float64\n",
      " 4   cyclic_enc__date_end_divider__month_Service Start Date_cos  742 non-null    Float64\n",
      " 5   cyclic_enc__date_end_divider__day_Service Start Date_sin    742 non-null    Float64\n",
      " 6   cyclic_enc__date_end_divider__day_Service Start Date_cos    742 non-null    Float64\n",
      " 7   cyclic_enc__date_start_divider__month_Service End Date_sin  742 non-null    Float64\n",
      " 8   cyclic_enc__date_start_divider__month_Service End Date_cos  742 non-null    Float64\n",
      " 9   cyclic_enc__date_start_divider__day_Service End Date_sin    742 non-null    Float64\n",
      " 10  cyclic_enc__date_start_divider__day_Service End Date_cos    742 non-null    Float64\n",
      " 11  remainder__other_charges_bool__Other Charges                742 non-null    boolean\n",
      " 12  remainder__date_end_divider__year_Service Start Date        742 non-null    string \n",
      " 13  remainder__date_start_divider__year_Service End Date        742 non-null    string \n",
      " 14  remainder__remainder__is_federal                            742 non-null    boolean\n",
      " 15  remainder__remainder__is_bws                                742 non-null    boolean\n",
      " 16  remainder__remainder__Meter AMR                             742 non-null    boolean\n",
      " 17  remainder__remainder__# days                                742 non-null    Int64  \n",
      " 18  remainder__remainder__Consumption (HCF)                     742 non-null    Int64  \n",
      " 19  remainder__remainder__Current Charges                       742 non-null    Float64\n",
      " 20  remainder__remainder__Estimated score                       742 non-null    Float64\n",
      " 21  remainder__remainder__Bill Analyzed score                   742 non-null    Float64\n",
      "dtypes: Float64(14), Int64(2), boolean(4), string(2)\n",
      "memory usage: 121.9 KB\n"
     ]
    }
   ],
   "source": [
    "agg_dataset_clustering.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare clustering dataset to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_col_names = [col[-1].lower().replace(' ', '_') for col in agg_dataset_clustering.columns.str.split('__')]\n",
    "\n",
    "agg_dataset_clustering = agg_dataset_clustering \\\n",
    "                        .rename(columns=pd.Series(real_col_names, index=agg_dataset_clustering.columns).to_dict()) \\\n",
    "                        .rename(columns={\n",
    "                            'account_name': 'development_type',\n",
    "                            'building': 'location_category',\n",
    "                            'meter_amr': 'is_meter_amr',\n",
    "                            'other_charges': 'is_only_water_sewer_charges'\n",
    "                        }) \\\n",
    "                        .drop(columns=['is_bws'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dataset_clustering.to_csv('./data/cluster_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 742 entries, 0 to 741\n",
      "Data columns (total 14 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   boroughs_mapping__Borough          742 non-null    string \n",
      " 1   location_category__Building        742 non-null    string \n",
      " 2   development_type__Account Name     742 non-null    string \n",
      " 3   other_charges_bool__Other Charges  742 non-null    boolean\n",
      " 4   remainder__Service Start Date      742 non-null    string \n",
      " 5   remainder__is_federal              742 non-null    boolean\n",
      " 6   remainder__is_bws                  742 non-null    boolean\n",
      " 7   remainder__Meter AMR               742 non-null    boolean\n",
      " 8   remainder__# days                  742 non-null    Int64  \n",
      " 9   remainder__Consumption (HCF)       742 non-null    Int64  \n",
      " 10  remainder__Current Charges         742 non-null    Float64\n",
      " 11  remainder__Service End Date        742 non-null    string \n",
      " 12  remainder__Estimated score         742 non-null    Float64\n",
      " 13  remainder__Bill Analyzed score     742 non-null    Float64\n",
      "dtypes: Float64(3), Int64(2), boolean(4), string(5)\n",
      "memory usage: 67.5 KB\n"
     ]
    }
   ],
   "source": [
    "agg_dataset_association.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare association rules dataset to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_col_names = [col[-1].lower().replace(' ', '_') for col in agg_dataset_association.columns.str.split('__')]\n",
    "\n",
    "agg_dataset_association = agg_dataset_association \\\n",
    "                        .rename(columns=pd.Series(real_col_names, index=agg_dataset_association.columns).to_dict()) \\\n",
    "                        .rename(columns={\n",
    "                            'account_name': 'development_type',\n",
    "                            'building': 'location_category',\n",
    "                            'meter_amr': 'is_meter_amr',\n",
    "                            'other_charges': 'is_only_water_sewer_charges'\n",
    "                            \n",
    "                        }) \\\n",
    "                        .drop(columns=['is_bws'])\n",
    "\n",
    "agg_dataset_association.to_csv('./data/association_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
